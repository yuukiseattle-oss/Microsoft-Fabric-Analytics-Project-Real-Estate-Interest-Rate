{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da6d4c-ee3b-47ec-b29a-c92697220b83",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":\"2026-02-19T13:43:10.2556661Z\",\"execution_start_time\":\"2026-02-19T13:43:29.1127194Z\",\"execution_finish_time\":\"2026-02-19T13:43:29.434297Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:42:39.5672647Z",
       "execution_start_time": "2026-02-22T04:42:39.289154Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f56cfa00-937f-4ed4-98fe-942c08d5c95e",
       "queued_time": "2026-02-22T04:42:39.2879782Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 84,
       "statement_ids": [
        84
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 84, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c07e42f-7992-4c58-a71e-474e4b745267",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:23:15.2726967Z\",\"execution_finish_time\":\"2026-02-15T10:23:16.684829Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:42:58.848148Z",
       "execution_start_time": "2026-02-22T04:42:58.1196254Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e4d1f607-0674-44c8-a417-5c9a93fecd39",
       "queued_time": "2026-02-22T04:42:58.1184988Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 88,
       "statement_ids": [
        88
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 88, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>1Y</th>\n",
       "      <th>2Y</th>\n",
       "      <th>3Y</th>\n",
       "      <th>4Y</th>\n",
       "      <th>5Y</th>\n",
       "      <th>6Y</th>\n",
       "      <th>7Y</th>\n",
       "      <th>8Y</th>\n",
       "      <th>9Y</th>\n",
       "      <th>10Y</th>\n",
       "      <th>15Y</th>\n",
       "      <th>20Y</th>\n",
       "      <th>25Y</th>\n",
       "      <th>30Y</th>\n",
       "      <th>40Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1974-09-24</td>\n",
       "      <td>10.327</td>\n",
       "      <td>9.362</td>\n",
       "      <td>8.830</td>\n",
       "      <td>8.515</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.121</td>\n",
       "      <td>8.127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974-09-25</td>\n",
       "      <td>10.333</td>\n",
       "      <td>9.364</td>\n",
       "      <td>8.831</td>\n",
       "      <td>8.516</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.121</td>\n",
       "      <td>8.127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1974-09-26</td>\n",
       "      <td>10.340</td>\n",
       "      <td>9.366</td>\n",
       "      <td>8.832</td>\n",
       "      <td>8.516</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1974-09-27</td>\n",
       "      <td>10.347</td>\n",
       "      <td>9.367</td>\n",
       "      <td>8.833</td>\n",
       "      <td>8.517</td>\n",
       "      <td>8.349</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1974-09-28</td>\n",
       "      <td>10.354</td>\n",
       "      <td>9.369</td>\n",
       "      <td>8.834</td>\n",
       "      <td>8.518</td>\n",
       "      <td>8.349</td>\n",
       "      <td>8.291</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      1Y     2Y     3Y     4Y     5Y     6Y    7Y     8Y     9Y  \\\n",
       "0 1974-09-24  10.327  9.362  8.830  8.515  8.348  8.290  8.24  8.121  8.127   \n",
       "1 1974-09-25  10.333  9.364  8.831  8.516  8.348  8.290  8.24  8.121  8.127   \n",
       "2 1974-09-26  10.340  9.366  8.832  8.516  8.348  8.290  8.24  8.122  8.128   \n",
       "3 1974-09-27  10.347  9.367  8.833  8.517  8.349  8.290  8.24  8.122  8.128   \n",
       "4 1974-09-28  10.354  9.369  8.834  8.518  8.349  8.291  8.24  8.122  8.129   \n",
       "\n",
       "   10Y  15Y  20Y  25Y  30Y  40Y  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = \"https://www.mof.go.jp/english/policy/jgbs/reference/interest_rate/historical/jgbcme_all.csv\"\n",
    "response = requests.get(url, timeout=30)\n",
    "response.encoding = 'shift_jis'\n",
    "\n",
    "df = pd.read_csv(StringIO(response.text), header=1, na_values=\"-\",parse_dates=[\"Date\"])\n",
    "df.head(5)\n",
    "# 10年国債利回りを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fca4f-1b5f-4ce3-a2f5-b415ec8cca01",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:23:19.0255454Z\",\"execution_finish_time\":\"2026-02-15T10:23:43.0770853Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 2: pandas DataFrame → Spark DataFrame → Delta table 書き込み\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, TimestampType, IntegerType\n",
    "\n",
    "# --- 明示的スキーマ定義（設計書準拠）---\n",
    "schema = StructType([\n",
    "    StructField(\"date\",        DateType(),      nullable=False),\n",
    "    StructField(\"yield_10y\",   DoubleType(),    nullable=False),\n",
    "    StructField(\"ingested_at\", TimestampType(), nullable=False),\n",
    "    StructField(\"year\",        IntegerType(),   nullable=False),\n",
    "    StructField(\"month\",       IntegerType(),   nullable=False),\n",
    "])\n",
    "\n",
    "# --- スキーマ定義と同じ順序でカラムを並べる（位置マッピングのずれを防止）---\n",
    "df_ordered = df[[\"date\", \"yield_10y\", \"ingested_at\", \"year\", \"month\"]]\n",
    "\n",
    "# --- pandas → Spark DataFrame 変換 ---\n",
    "spark_df = spark.createDataFrame(df_ordered, schema=schema)\n",
    "\n",
    "# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\n",
    "(\n",
    "    spark_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(TABLE_NAME)\n",
    ")\n",
    "\n",
    "actual_count = spark.read.table(TABLE_NAME).count()\n",
    "print(f\"[完了] Delta table '{TABLE_NAME}' に {actual_count:,} 行を書き込みました\")\n",
    "print(f\"パーティション: year / month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102da8f-e16c-4880-a4d5-a20e4d21396f",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:27:40.0943652Z\",\"execution_finish_time\":\"2026-02-15T10:27:41.4843815Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:22:23.5780253Z",
       "execution_start_time": "2026-02-22T04:21:57.3535121Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c34445a7-f390-44ce-8621-ced3f08fb634",
       "queued_time": "2026-02-22T04:21:57.0301172Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2026-02-22T04:22:22.654GMT",
          "dataRead": 103756,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5))",
          "displayName": "getRowsInJsonString at Display.scala:422",
          "jobGroup": "6",
          "jobId": 14,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:422",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 1644,
          "stageIds": [
           22
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:22.240GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:22.044GMT",
          "dataRead": 4760,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 13,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           19,
           20,
           21
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:21.889GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:21.815GMT",
          "dataRead": 5917,
          "dataWritten": 4760,
          "description": "Delta: Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 12,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 60,
          "stageIds": [
           17,
           18
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:20.621GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:20.212GMT",
          "dataRead": 5469,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 11,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 11,
          "stageIds": [
           15,
           16
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:17.033GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:15.860GMT",
          "dataRead": 10212,
          "dataWritten": 5469,
          "description": "Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 10,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 22,
          "stageIds": [
           14
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:15.634GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:13.618GMT",
          "dataRead": 10212,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5))",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 9,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 11,
          "stageIds": [
           13
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:13.064GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 6,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "4aee22ec-fa7e-4fa1-876c-9a53000b7a7d",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 4aee22ec-fa7e-4fa1-876c-9a53000b7a7d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pandas DataFrame → Spark DataFrame に変換し、Delta table として書き込み\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "table_name = \"bronze_interest_rate\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"dbo.{table_name}\")\n",
    "\n",
    "print(f\"Delta table '{table_name}' に {spark_df.count()} 行を書き込みました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3116a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T05:01:01.3646038Z",
       "execution_start_time": "2026-02-22T05:01:01.0770949Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a30135fd-760a-4b5b-9d77-2166d7978401",
       "queued_time": "2026-02-22T05:01:01.075905Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 200,
       "statement_ids": [
        200
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 200, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---型変換---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, IntegerType, TimestampType \n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"10Y\", DoubleType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"ingested_at\", TimestampType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32043831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T05:01:01.8667614Z",
       "execution_start_time": "2026-02-22T05:01:01.5886155Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "80ec02a9-3cee-471e-b0a8-a839349306a9",
       "queued_time": "2026-02-22T05:01:01.5874226Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 201,
       "statement_ids": [
        201
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 201, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported cast from double to timestamp using function cast_timestamp\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `9.362` in type `float`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[602], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m TABLE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_interest_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --- pandas → Spark DataFrame 変換 ---\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\u001b[39;00m\n\u001b[1;32m      6\u001b[0m (\n\u001b[1;32m      7\u001b[0m     spark_df\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39msaveAsTable(TABLE_NAME)\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     verify_func(obj)\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2174\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2174\u001b[0m         verifier(v)\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2176\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2090\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_integer\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2089\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2090\u001b[0m     verify_acceptable_types(obj)\n\u001b[1;32m   2091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2147483648\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2147483647\u001b[39m:\n\u001b[1;32m   2092\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2093\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_OUT_OF_BOUND\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2094\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             },\n\u001b[1;32m   2100\u001b[0m         )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2020\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2020\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2021\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2022\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2023\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2024\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2025\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2026\u001b[0m             },\n\u001b[1;32m   2027\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `9.362` in type `float`."
     ]
    }
   ],
   "source": [
    "TABLE_NAME = \"bronze_interest_rate\"\n",
    "# --- pandas → Spark DataFrame 変換 ---\n",
    "spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\n",
    "(\n",
    "    spark_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(TABLE_NAME)\n",
    ")\n",
    "\n",
    "actual_count = spark.read.table(TABLE_NAME).count()\n",
    "print(f\"[完了] Delta table '{TABLE_NAME}' に {actual_count:,} 行を書き込みました\")\n",
    "print(f\"パーティション: year / month\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bb192-016c-476f-8c01-2e2023e9952c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fl5t3dg48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:20:15.2248499Z",
       "execution_start_time": "2026-02-22T06:20:14.8595824Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "59990cc4-9b03-4a56-a3b3-3d8c4f1e2e79",
       "queued_time": "2026-02-22T06:19:52.0433764Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2018, 1), (2018, 2), (2018, 3), (2018, 4), (2019, 1), (2019, 2), (2019, 3), (2019, 4), (2020, 1), (2020, 2), (2020, 3), (2020, 4), (2021, 1), (2021, 2), (2021, 3), (2021, 4), (2022, 1), (2022, 2), (2022, 3), (2022, 4), (2023, 1), (2023, 2), (2023, 3), (2023, 4), (2024, 1), (2024, 2), (2024, 3), (2024, 4)]\n",
      "取得設定:\n",
      "  都道府県        : ['13'] （東京都）\n",
      "  期間（年×四半期）: 28 件  ((2018, 1) 〜 (2024, 4))\n",
      "  総 API コール数  : 28 回\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 4: 設定 - 不動産取引データ取得（国土交通省 不動産情報ライブラリAPI）\n",
    "# データソース: https://www.reinfolib.mlit.go.jp/ex-api/external/XIT001\n",
    "# 事前準備: reinfolib の API キーを取得すること\n",
    "#   https://www.reinfolib.mlit.go.jp/\n",
    "# ============================================================\n",
    "\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# --- APIキー設定 ---\n",
    "# 【推奨】Fabric Key Vault から取得:\n",
    "#   KEY_VAULT_URL = \"https://your-keyvault.vault.azure.net/\"\n",
    "#   API_KEY = notebookutils.credentials.getSecret(KEY_VAULT_URL, \"reinfolib-api-key\")\n",
    "API_KEY = \"d0237ca0d02d4a6095038c68829c0ad5\"  # ← Fabric 上でここに API キーを入力すること\n",
    "\n",
    "# --- 取得設定 ---\n",
    "BASE_URL   = \"https://www.reinfolib.mlit.go.jp/ex-api/external/XIT001\"\n",
    "TABLE_NAME = \"bronze_realestate_transactions\"\n",
    "\n",
    "# 都道府県コード（テスト: 東京都のみ。全国展開時は PREFECTURES_ALL に切り替え）\n",
    "PREFECTURES     = [\"13\"]                               # 東京都のみ\n",
    "PREFECTURES_ALL = [f\"{i:02d}\" for i in range(1, 48)]  # 全国展開時に切り替え\n",
    "\n",
    "# 取得期間: 2020Q1〜2024Q4（分析対象全期間）\n",
    "YEAR_QUARTERS = [(year, q) for year in range(2018, 2025) for q in range(1, 5)]\n",
    "print(YEAR_QUARTERS)\n",
    "\n",
    "print(f\"取得設定:\")\n",
    "print(f\"  都道府県        : {PREFECTURES} （東京都）\")\n",
    "print(f\"  期間（年×四半期）: {len(YEAR_QUARTERS)} 件  ({YEAR_QUARTERS[0]} 〜 {YEAR_QUARTERS[-1]})\")\n",
    "print(f\"  総 API コール数  : {len(PREFECTURES) * len(YEAR_QUARTERS):,} 回\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q8e3oh4cbmm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:32:57.5750742Z",
       "execution_start_time": "2026-02-22T06:32:57.2796276Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "66e07585-1969-4141-8081-bc63975db37f",
       "queued_time": "2026-02-22T06:32:45.829838Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": "2026-02-22T06:32:45.8309353Z",
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch_transactions 関数 定義完了\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 5: API取得関数定義（リトライ・エラーハンドリング付き）\n",
    "# ============================================================\n",
    "\n",
    "def fetch_transactions(year: int, quarter: int, area: str, api_key: str,\n",
    "                       max_retries: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    国土交通省 不動産情報ライブラリ API から取引データを取得する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year        : 取得年 (2020-2024)\n",
    "    quarter     : 四半期 (1-4)\n",
    "    area        : 都道府県コード (\"01\"-\"47\")\n",
    "    api_key     : Ocp-Apim-Subscription-Key\n",
    "    max_retries : エラー時の最大リトライ回数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list: 取引データのリスト（該当なし or エラー時は空リスト）\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"year\"                : year,\n",
    "        \"quarter\"             : quarter,\n",
    "        \"area\"                : area,\n",
    "        \"priceClassification\" : \"01\",   # 01=取引価格情報（成約価格は \"02\"）\n",
    "    }\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(BASE_URL, params=params, headers=headers, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            body = resp.json()\n",
    "            if body.get(\"status\") == \"OK\":\n",
    "                return body.get(\"data\", [])\n",
    "            else:\n",
    "                print(f\"  [WARN] status={body.get('status')} ({year}Q{quarter} area={area})\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                wait = 2 ** attempt   # Exponential backoff: 2s → 4s → 8s\n",
    "                print(f\"  [RETRY {attempt}/{max_retries}] {year}Q{quarter} area={area} → {e} ({wait}s 待機)\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"  [ERROR] {year}Q{quarter} area={area} → {e} (スキップ)\")\n",
    "                return []\n",
    "\n",
    "print(\"fetch_transactions 関数 定義完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c24kpjwj3n",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:44:37.6511988Z",
       "execution_start_time": "2026-02-22T06:44:12.591229Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b2bfb703-ba07-4b96-8636-aac322a0df25",
       "queued_time": "2026-02-22T06:44:12.5899675Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018Q1]  8,101 件  累計:   8,101 件  [1/28]\n",
      "[2018Q2]  7,762 件  累計:  15,863 件  [2/28]\n",
      "[2018Q3]  7,848 件  累計:  23,711 件  [3/28]\n",
      "[2018Q4]  7,958 件  累計:  31,669 件  [4/28]\n",
      "[2019Q1]  8,056 件  累計:  39,725 件  [5/28]\n",
      "[2019Q2]  7,832 件  累計:  47,557 件  [6/28]\n",
      "[2019Q3]  7,817 件  累計:  55,374 件  [7/28]\n",
      "[2019Q4]  7,673 件  累計:  63,047 件  [8/28]\n",
      "[2020Q1]  9,242 件  累計:  72,289 件  [9/28]\n",
      "[2020Q2]  6,749 件  累計:  79,038 件  [10/28]\n",
      "[2020Q3]  9,716 件  累計:  88,754 件  [11/28]\n",
      "[2020Q4] 10,063 件  累計:  98,817 件  [12/28]\n",
      "[2021Q1]  9,572 件  累計: 108,389 件  [13/28]\n",
      "[2021Q2]  9,282 件  累計: 117,671 件  [14/28]\n",
      "[2021Q3]  8,682 件  累計: 126,353 件  [15/28]\n",
      "[2021Q4]  8,688 件  累計: 135,041 件  [16/28]\n",
      "[2022Q1]  8,263 件  累計: 143,304 件  [17/28]\n",
      "[2022Q2]  8,069 件  累計: 151,373 件  [18/28]\n",
      "[2022Q3]  8,085 件  累計: 159,458 件  [19/28]\n",
      "[2022Q4]  7,952 件  累計: 167,410 件  [20/28]\n",
      "[2023Q1]  7,618 件  累計: 175,028 件  [21/28]\n",
      "[2023Q2]  7,822 件  累計: 182,850 件  [22/28]\n",
      "[2023Q3]  8,178 件  累計: 191,028 件  [23/28]\n",
      "[2023Q4]  8,051 件  累計: 199,079 件  [24/28]\n",
      "[2024Q1]  8,184 件  累計: 207,263 件  [25/28]\n",
      "[2024Q2]  8,292 件  累計: 215,555 件  [26/28]\n",
      "[2024Q3]  8,203 件  累計: 223,758 件  [27/28]\n",
      "[2024Q4]  8,052 件  累計: 231,810 件  [28/28]\n",
      "\n",
      "取得完了: 総レコード数 231,810 件\n",
      "[{'PriceCategory': '不動産取引価格情報', 'Type': '中古マンション等', 'Region': '', 'MunicipalityCode': '13101', 'Prefecture': '東京都', 'Municipality': '千代田区', 'DistrictName': '岩本町', 'TradePrice': '40000000', 'PricePerUnit': '', 'FloorPlan': '１Ｋ', 'Area': '25', 'UnitPrice': '', 'LandShape': '', 'Frontage': '', 'TotalFloorArea': '', 'BuildingYear': '2018年', 'Structure': 'ＲＣ', 'Use': '住宅', 'Purpose': '住宅', 'Direction': '', 'Classification': '', 'Breadth': '', 'CityPlanning': '商業地域', 'CoverageRatio': '80', 'FloorAreaRatio': '600', 'Period': '2024年第4四半期', 'Renovation': '未改装', 'Remarks': '', 'DistrictCode': '131010030', '_year': 2024, '_quarter': 4, '_ingested_at': datetime.datetime(2026, 2, 22, 6, 44, 13, 104063, tzinfo=datetime.timezone.utc)}, {'PriceCategory': '不動産取引価格情報', 'Type': '中古マンション等', 'Region': '', 'MunicipalityCode': '13101', 'Prefecture': '東京都', 'Municipality': '千代田区', 'DistrictName': '岩本町', 'TradePrice': '34000000', 'PricePerUnit': '', 'FloorPlan': '１Ｋ', 'Area': '30', 'UnitPrice': '', 'LandShape': '', 'Frontage': '', 'TotalFloorArea': '', 'BuildingYear': '2005年', 'Structure': 'ＲＣ', 'Use': '住宅', 'Purpose': '住宅', 'Direction': '', 'Classification': '', 'Breadth': '', 'CityPlanning': '商業地域', 'CoverageRatio': '80', 'FloorAreaRatio': '600', 'Period': '2024年第4四半期', 'Renovation': '未改装', 'Remarks': '', 'DistrictCode': '131010030', '_year': 2024, '_quarter': 4, '_ingested_at': datetime.datetime(2026, 2, 22, 6, 44, 13, 104063, tzinfo=datetime.timezone.utc)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 6: データ取得メインループ\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ingested_at  = datetime.now(timezone.utc)\n",
    "all_records  = []\n",
    "error_count  = 0\n",
    "total_calls  = len(PREFECTURES) * len(YEAR_QUARTERS)\n",
    "call_count   = 0\n",
    "\n",
    "for year, quarter in YEAR_QUARTERS:\n",
    "    yq_records = []\n",
    "\n",
    "    for area in PREFECTURES:\n",
    "        call_count += 1\n",
    "        records = fetch_transactions(year, quarter, area, API_KEY)\n",
    "\n",
    "        for r in records:\n",
    "            r[\"_year\"]        = year\n",
    "            r[\"_quarter\"]     = quarter\n",
    "            r[\"_ingested_at\"] = ingested_at\n",
    "\n",
    "        yq_records.extend(records)\n",
    "        time.sleep(0.3)   # レート制限対策\n",
    "\n",
    "    all_records.extend(yq_records)\n",
    "    print(f\"[{year}Q{quarter}] {len(yq_records):>6,} 件  累計: {len(all_records):>7,} 件  [{call_count}/{total_calls}]\")\n",
    "\n",
    "print(f\"\\n取得完了: 総レコード数 {len(all_records):,} 件\")\n",
    "print(yq_records[:2])  # 取得したレコードのサンプルを表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ayrk4b6bf8w",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:54:59.2649779Z",
       "execution_start_time": "2026-02-22T06:54:19.8681304Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5e478581-44a4-4bd8-832e-6cb53d26abbd",
       "queued_time": "2026-02-22T06:54:19.8667912Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2026-02-22T06:54:57.582GMT",
          "dataRead": 3095,
          "dataWritten": 0,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "15",
          "jobId": 19,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           27,
           28,
           26
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:57.541GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:57.517GMT",
          "dataRead": 35247,
          "dataWritten": 3095,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "15",
          "jobId": 18,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 81,
          "stageIds": [
           24,
           25
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:56.155GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:55.409GMT",
          "dataRead": 79197,
          "dataWritten": 35247,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "15",
          "jobId": 17,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 62,
          "stageIds": [
           23
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:54.994GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:49.456GMT",
          "dataRead": 79197,
          "dataWritten": 0,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "15",
          "jobId": 16,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 31,
          "stageIds": [
           22
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:49.116GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:48.586GMT",
          "dataRead": 8617,
          "dataWritten": 0,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$submit$1 at FutureTask.java:264",
          "jobGroup": "15",
          "jobId": 15,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$submit$1 at FutureTask.java:264",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 8,
          "numTasks": 9,
          "rowCount": 8,
          "stageIds": [
           20,
           21
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:47.825GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:48.396GMT",
          "dataRead": 185483,
          "dataWritten": 0,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$submit$1 at FutureTask.java:264",
          "jobGroup": "15",
          "jobId": 14,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$submit$1 at FutureTask.java:264",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 8,
          "numTasks": 9,
          "rowCount": 248,
          "stageIds": [
           19,
           18
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:46.252GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:47.762GMT",
          "dataRead": 0,
          "dataWritten": 8617,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$submit$1 at FutureTask.java:264",
          "jobGroup": "15",
          "jobId": 13,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$submit$1 at FutureTask.java:264",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 8,
          "numCompletedStages": 1,
          "numCompletedTasks": 8,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 8,
          "rowCount": 8,
          "stageIds": [
           17
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:43.668GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:46.182GMT",
          "dataRead": 0,
          "dataWritten": 185483,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$submit$1 at FutureTask.java:264",
          "jobGroup": "15",
          "jobId": 12,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$submit$1 at FutureTask.java:264",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 8,
          "numCompletedStages": 1,
          "numCompletedTasks": 8,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 8,
          "rowCount": 248,
          "stageIds": [
           16
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:43.417GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:43.056GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "jobGroup": "15",
          "jobId": 11,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 0,
          "numCompletedStages": 0,
          "numCompletedTasks": 0,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 0,
          "rowCount": 0,
          "stageIds": [],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:43.056GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:42.739GMT",
          "dataRead": 9829756,
          "dataWritten": 12897452,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "15",
          "jobId": 10,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 27,
          "numCompletedStages": 1,
          "numCompletedTasks": 27,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 24,
          "numTasks": 51,
          "rowCount": 231810,
          "stageIds": [
           15,
           14
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:34.325GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:54:34.272GMT",
          "dataRead": 0,
          "dataWritten": 28400843,
          "description": "Job group for statement 15:\n\n# ============================================================\n# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n# ============================================================\n\n# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\nCOLUMN_MAP = {\n    \"PriceCategory\"  : \"price_category\",\n    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n    \"Region\"         : \"region\",\n    \"MunicipalityCode\": \"municipality_code\",\n    \"Prefecture\"     : \"prefecture\",\n    \"Municipality\"   : \"municipality\",\n    \"DistrictName\"   : \"district\",\n    \"TradePrice\"     : \"trade_price\",\n    \"PricePerUnit\"   : \"price_per_unit\",\n    \"FloorPlan\"      : \"floor_plan\",\n    \"Area\"           : \"area\",\n    \"UnitPrice\"      : \"unit_price\",\n    \"LandShape\"      : \"land_shape\",\n    \"Frontage\"       : \"frontage\",\n    \"TotalFloorArea\" : \"total_floor_area\",\n    \"BuildingYear\"   : \"building_year\",\n    \"Structure\"      : \"structure\",\n    \"Use\"            : \"use\",\n    \"Purpose\"        : \"purpose\",\n    \"Di...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "15",
          "jobId": 9,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 24,
          "numCompletedStages": 1,
          "numCompletedTasks": 24,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 24,
          "rowCount": 231810,
          "stageIds": [
           13
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:54:30.249GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 11,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整形後カラム数: 33\n",
      "整形後レコード数: 231,810 件\n",
      "\n",
      "[完了] Delta table 'bronze_realestate_transactions' に 231,810 行を書き込みました\n",
      "パーティション: year / quarter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 7: pandas DataFrame 整形 → Delta table 書き込み\n",
    "# API レスポンスの英語キー名をスネークケースに変換し、メタデータ列を追加する\n",
    "# ============================================================\n",
    "\n",
    "# --- カラム名マッピング（API英語キー → Bronze テーブル列名）---\n",
    "COLUMN_MAP = {\n",
    "    \"PriceCategory\"  : \"price_category\",\n",
    "    \"Type\"           : \"property_type\",       # 中古マンション等, 宅地(土地と建物) 等\n",
    "    \"Region\"         : \"region\",\n",
    "    \"MunicipalityCode\": \"municipality_code\",\n",
    "    \"Prefecture\"     : \"prefecture\",\n",
    "    \"Municipality\"   : \"municipality\",\n",
    "    \"DistrictName\"   : \"district\",\n",
    "    \"TradePrice\"     : \"trade_price\",\n",
    "    \"PricePerUnit\"   : \"price_per_unit\",\n",
    "    \"FloorPlan\"      : \"floor_plan\",\n",
    "    \"Area\"           : \"area\",\n",
    "    \"UnitPrice\"      : \"unit_price\",\n",
    "    \"LandShape\"      : \"land_shape\",\n",
    "    \"Frontage\"       : \"frontage\",\n",
    "    \"TotalFloorArea\" : \"total_floor_area\",\n",
    "    \"BuildingYear\"   : \"building_year\",\n",
    "    \"Structure\"      : \"structure\",\n",
    "    \"Use\"            : \"use\",\n",
    "    \"Purpose\"        : \"purpose\",\n",
    "    \"Direction\"      : \"direction\",\n",
    "    \"Classification\" : \"classification\",\n",
    "    \"Breadth\"        : \"breadth\",\n",
    "    \"CityPlanning\"   : \"city_planning\",\n",
    "    \"CoverageRatio\"  : \"coverage_ratio\",\n",
    "    \"FloorAreaRatio\" : \"floor_area_ratio\",\n",
    "    \"Period\"         : \"transaction_period\",  # 例: \"令和２年第１四半期\"\n",
    "    \"Renovation\"     : \"renovation\",\n",
    "    \"Remarks\"        : \"remarks\",\n",
    "    \"_year\"          : \"year\",\n",
    "    \"_quarter\"       : \"quarter\",\n",
    "    \"_ingested_at\"   : \"ingested_at\",\n",
    "}\n",
    "\n",
    "# --- pandas DataFrame に変換・整形 ---\n",
    "df_bronze = pd.DataFrame(all_records).rename(columns=COLUMN_MAP)\n",
    "\n",
    "# transaction_id（UUID）を先頭列に追加\n",
    "df_bronze.insert(0, \"transaction_id\", [str(uuid.uuid4()) for _ in range(len(df_bronze))])\n",
    "\n",
    "print(f\"整形後カラム数: {len(df_bronze.columns)}\")\n",
    "print(f\"整形後レコード数: {len(df_bronze):,} 件\")\n",
    "\n",
    "# --- pandas → Spark DataFrame → Delta table 書き込み ---\n",
    "spark_df = spark.createDataFrame(df_bronze)\n",
    "\n",
    "(\n",
    "    spark_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"quarter\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(TABLE_NAME)\n",
    ")\n",
    "\n",
    "actual_count = spark.read.table(TABLE_NAME).count()\n",
    "print(f\"\\n[完了] Delta table '{TABLE_NAME}' に {actual_count:,} 行を書き込みました\")\n",
    "print(f\"パーティション: year / quarter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crhp8hpbg1u",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:55:10.0338121Z",
       "execution_start_time": "2026-02-22T06:55:03.9186713Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4f9054ea-a0e0-4455-a4f9-3e03bc2dd907",
       "queued_time": "2026-02-22T06:55:03.91741Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2026-02-22T06:55:08.808GMT",
          "dataRead": 491695,
          "dataWritten": 0,
          "description": "Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n",
          "displayName": "getRowsInJsonString at Display.scala:422",
          "jobGroup": "16",
          "jobId": 28,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:422",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 4096,
          "stageIds": [
           44
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:08.633GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:08.515GMT",
          "dataRead": 26356,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "16",
          "jobId": 27,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 28,
          "stageIds": [
           42,
           43
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:08.245GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:07.596GMT",
          "dataRead": 2775,
          "dataWritten": 0,
          "description": "Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "16",
          "jobId": 26,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 7,
          "numTasks": 8,
          "rowCount": 35,
          "stageIds": [
           40,
           41
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:07.544GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:07.493GMT",
          "dataRead": 84484,
          "dataWritten": 2775,
          "description": "Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "16",
          "jobId": 25,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 7,
          "numCompletedStages": 1,
          "numCompletedTasks": 7,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 7,
          "rowCount": 231845,
          "stageIds": [
           39
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:06.899GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:06.656GMT",
          "dataRead": 1829,
          "dataWritten": 0,
          "description": "Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "16",
          "jobId": 24,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 7,
          "numTasks": 8,
          "rowCount": 28,
          "stageIds": [
           37,
           38
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:06.567GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:06.500GMT",
          "dataRead": 0,
          "dataWritten": 1829,
          "description": "Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "16",
          "jobId": 23,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 7,
          "numCompletedStages": 1,
          "numCompletedTasks": 7,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 7,
          "rowCount": 231838,
          "stageIds": [
           36
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:05.874GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:05.599GMT",
          "dataRead": 35464,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "16",
          "jobId": 22,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 30,
          "stageIds": [
           34,
           35
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:05.362GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:05.194GMT",
          "dataRead": 5322,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n: Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "16",
          "jobId": 21,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           33,
           31,
           32
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:05.095GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T06:55:05.060GMT",
          "dataRead": 35464,
          "dataWritten": 5322,
          "description": "Delta: Delta: Job group for statement 16:\n\n# ============================================================\n# Cell 8: 動作確認\n# ============================================================\nfrom pyspark.sql.functions import col, count, min, max\n\ndf_check = spark.read.table(TABLE_NAME)\n\nprint(\"=== スキーマ ===\")\ndf_check.printSchema()\n\nprint(\"=== 件数・期間確認（年×四半期別）===\")\ndf_check.groupBy(\"year\", \"quarter\")         .count()         .orderBy(\"year\", \"quarter\")         .show(25)\n\nprint(\"=== 物件種別の内訳 ===\")\ndf_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n\nprint(\"=== サンプル（先頭5行）===\")\ndisplay(df_check.limit(5))\n: Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "16",
          "jobId": 20,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 80,
          "stageIds": [
           30,
           29
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:55:04.641GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 9,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== スキーマ ===\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- price_category: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- municipality_code: string (nullable = true)\n",
      " |-- prefecture: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- trade_price: string (nullable = true)\n",
      " |-- price_per_unit: string (nullable = true)\n",
      " |-- floor_plan: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      " |-- land_shape: string (nullable = true)\n",
      " |-- frontage: string (nullable = true)\n",
      " |-- total_floor_area: string (nullable = true)\n",
      " |-- building_year: string (nullable = true)\n",
      " |-- structure: string (nullable = true)\n",
      " |-- use: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- classification: string (nullable = true)\n",
      " |-- breadth: string (nullable = true)\n",
      " |-- city_planning: string (nullable = true)\n",
      " |-- coverage_ratio: string (nullable = true)\n",
      " |-- floor_area_ratio: string (nullable = true)\n",
      " |-- transaction_period: string (nullable = true)\n",
      " |-- renovation: string (nullable = true)\n",
      " |-- remarks: string (nullable = true)\n",
      " |-- DistrictCode: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- quarter: long (nullable = true)\n",
      " |-- ingested_at: timestamp (nullable = true)\n",
      "\n",
      "=== 件数・期間確認（年×四半期別）===\n",
      "+----+-------+-----+\n",
      "|year|quarter|count|\n",
      "+----+-------+-----+\n",
      "|2018|      1| 8101|\n",
      "|2018|      2| 7762|\n",
      "|2018|      3| 7848|\n",
      "|2018|      4| 7958|\n",
      "|2019|      1| 8056|\n",
      "|2019|      2| 7832|\n",
      "|2019|      3| 7817|\n",
      "|2019|      4| 7673|\n",
      "|2020|      1| 9242|\n",
      "|2020|      2| 6749|\n",
      "|2020|      3| 9716|\n",
      "|2020|      4|10063|\n",
      "|2021|      1| 9572|\n",
      "|2021|      2| 9282|\n",
      "|2021|      3| 8682|\n",
      "|2021|      4| 8688|\n",
      "|2022|      1| 8263|\n",
      "|2022|      2| 8069|\n",
      "|2022|      3| 8085|\n",
      "|2022|      4| 7952|\n",
      "|2023|      1| 7618|\n",
      "|2023|      2| 7822|\n",
      "|2023|      3| 8178|\n",
      "|2023|      4| 8051|\n",
      "|2024|      1| 8184|\n",
      "+----+-------+-----+\n",
      "only showing top 25 rows\n",
      "\n",
      "=== 物件種別の内訳 ===\n",
      "+----------------+------+\n",
      "|   property_type| count|\n",
      "+----------------+------+\n",
      "|中古マンション等|118941|\n",
      "|宅地(土地と建物)| 77415|\n",
      "|      宅地(土地)| 35093|\n",
      "|            林地|   281|\n",
      "|            農地|    80|\n",
      "+----------------+------+\n",
      "\n",
      "=== サンプル（先頭5行）===\n"
     ]
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "9f0d3285-d389-485b-bc83-d51e865b9163",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 9f0d3285-d389-485b-bc83-d51e865b9163)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Cell 8: 動作確認\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import col, count, min, max\n",
    "\n",
    "df_check = spark.read.table(TABLE_NAME)\n",
    "\n",
    "print(\"=== スキーマ ===\")\n",
    "df_check.printSchema()\n",
    "\n",
    "print(\"=== 件数・期間確認（年×四半期別）===\")\n",
    "df_check.groupBy(\"year\", \"quarter\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"year\", \"quarter\") \\\n",
    "        .show(25)\n",
    "\n",
    "print(\"=== 物件種別の内訳 ===\")\n",
    "df_check.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print(\"=== サンプル（先頭5行）===\")\n",
    "display(df_check.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a40a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T06:58:46.4623621Z",
       "execution_start_time": "2026-02-22T06:58:44.287814Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "482d0c52-7003-4bfc-acf1-b48c182d12db",
       "queued_time": "2026-02-22T06:58:44.2866039Z",
       "session_id": "781f4471-4ff3-469e-bfe5-8d26db501a9a",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2026-02-22T06:58:45.694GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 42:\ndisplay(df_check.orderBy(col(\"year\").asc(),col(\"quarter\").asc()).limit(10))",
          "displayName": "getRowsInJsonString at Display.scala:422",
          "jobGroup": "42",
          "jobId": 32,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:422",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 7,
          "numCompletedStages": 1,
          "numCompletedTasks": 7,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 7,
          "rowCount": 231810,
          "stageIds": [
           48
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T06:58:44.908GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 1,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 42,
       "statement_ids": [
        42
       ]
      },
      "text/plain": [
       "StatementMeta(, 781f4471-4ff3-469e-bfe5-8d26db501a9a, 42, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "67f9a9d1-cf34-417a-833f-70e5b75ab8c4",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 67f9a9d1-cf34-417a-833f-70e5b75ab8c4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_check.orderBy(col(\"year\").asc(),col(\"quarter\").asc()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "daafb4a7-f64d-4a0d-b5b6-a3f7d2e6f3ab",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "a0023aa2-9b7b-419b-bec4-ad2bc8e7db13",
    "known_lakehouses": [
     {
      "id": "daafb4a7-f64d-4a0d-b5b6-a3f7d2e6f3ab"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "36d7f2a3-89c7-4786-bb95-a79a13fbe5e4": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "R1.5.13",
         "1": "-0.158",
         "2": "-0.159",
         "3": "-0.163",
         "4": "-0.169",
         "5": "-0.164",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.09",
         "10": "-0.043",
         "11": "0.175",
         "12": "0.375",
         "13": "0.463",
         "14": "0.539",
         "15": "0.592"
        },
        {
         "0": "R1.5.14",
         "1": "-0.159",
         "2": "-0.159",
         "3": "-0.163",
         "4": "-0.17",
         "5": "-0.164",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.094",
         "10": "-0.048",
         "11": "0.17",
         "12": "0.365",
         "13": "0.456",
         "14": "0.534",
         "15": "0.588"
        },
        {
         "0": "R1.5.15",
         "1": "-0.16",
         "2": "-0.16",
         "3": "-0.163",
         "4": "-0.17",
         "5": "-0.165",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.094",
         "10": "-0.048",
         "11": "0.17",
         "12": "0.365",
         "13": "0.453",
         "14": "0.53",
         "15": "0.588"
        },
        {
         "0": "R1.5.16",
         "1": "-0.16",
         "2": "-0.166",
         "3": "-0.174",
         "4": "-0.179",
         "5": "-0.175",
         "6": "-0.177",
         "7": "-0.169",
         "8": "-0.145",
         "9": "-0.104",
         "10": "-0.058",
         "11": "0.154",
         "12": "0.35",
         "13": "0.437",
         "14": "0.511",
         "15": "0.567"
        },
        {
         "0": "R1.5.17",
         "1": "-0.161",
         "2": "-0.164",
         "3": "-0.169",
         "4": "-0.175",
         "5": "-0.169",
         "6": "-0.172",
         "7": "-0.161",
         "8": "-0.137",
         "9": "-0.099",
         "10": "-0.053",
         "11": "0.165",
         "12": "0.356",
         "13": "0.447",
         "14": "0.525",
         "15": "0.582"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "基準日",
         "type": "string"
        },
        {
         "key": "1",
         "name": "1年",
         "type": "string"
        },
        {
         "key": "2",
         "name": "2年",
         "type": "string"
        },
        {
         "key": "3",
         "name": "3年",
         "type": "string"
        },
        {
         "key": "4",
         "name": "4年",
         "type": "double"
        },
        {
         "key": "5",
         "name": "5年",
         "type": "double"
        },
        {
         "key": "6",
         "name": "6年",
         "type": "double"
        },
        {
         "key": "7",
         "name": "7年",
         "type": "double"
        },
        {
         "key": "8",
         "name": "8年",
         "type": "double"
        },
        {
         "key": "9",
         "name": "9年",
         "type": "double"
        },
        {
         "key": "10",
         "name": "10年",
         "type": "string"
        },
        {
         "key": "11",
         "name": "15年",
         "type": "string"
        },
        {
         "key": "12",
         "name": "20年",
         "type": "string"
        },
        {
         "key": "13",
         "name": "25年",
         "type": "string"
        },
        {
         "key": "14",
         "name": "30年",
         "type": "string"
        },
        {
         "key": "15",
         "name": "40年",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
