{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da6d4c-ee3b-47ec-b29a-c92697220b83",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":\"2026-02-19T13:43:10.2556661Z\",\"execution_start_time\":\"2026-02-19T13:43:29.1127194Z\",\"execution_finish_time\":\"2026-02-19T13:43:29.434297Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:42:39.5672647Z",
       "execution_start_time": "2026-02-22T04:42:39.289154Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f56cfa00-937f-4ed4-98fe-942c08d5c95e",
       "queued_time": "2026-02-22T04:42:39.2879782Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 84,
       "statement_ids": [
        84
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 84, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c07e42f-7992-4c58-a71e-474e4b745267",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:23:15.2726967Z\",\"execution_finish_time\":\"2026-02-15T10:23:16.684829Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:42:58.848148Z",
       "execution_start_time": "2026-02-22T04:42:58.1196254Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e4d1f607-0674-44c8-a417-5c9a93fecd39",
       "queued_time": "2026-02-22T04:42:58.1184988Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 88,
       "statement_ids": [
        88
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 88, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>1Y</th>\n",
       "      <th>2Y</th>\n",
       "      <th>3Y</th>\n",
       "      <th>4Y</th>\n",
       "      <th>5Y</th>\n",
       "      <th>6Y</th>\n",
       "      <th>7Y</th>\n",
       "      <th>8Y</th>\n",
       "      <th>9Y</th>\n",
       "      <th>10Y</th>\n",
       "      <th>15Y</th>\n",
       "      <th>20Y</th>\n",
       "      <th>25Y</th>\n",
       "      <th>30Y</th>\n",
       "      <th>40Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1974-09-24</td>\n",
       "      <td>10.327</td>\n",
       "      <td>9.362</td>\n",
       "      <td>8.830</td>\n",
       "      <td>8.515</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.121</td>\n",
       "      <td>8.127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974-09-25</td>\n",
       "      <td>10.333</td>\n",
       "      <td>9.364</td>\n",
       "      <td>8.831</td>\n",
       "      <td>8.516</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.121</td>\n",
       "      <td>8.127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1974-09-26</td>\n",
       "      <td>10.340</td>\n",
       "      <td>9.366</td>\n",
       "      <td>8.832</td>\n",
       "      <td>8.516</td>\n",
       "      <td>8.348</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1974-09-27</td>\n",
       "      <td>10.347</td>\n",
       "      <td>9.367</td>\n",
       "      <td>8.833</td>\n",
       "      <td>8.517</td>\n",
       "      <td>8.349</td>\n",
       "      <td>8.290</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1974-09-28</td>\n",
       "      <td>10.354</td>\n",
       "      <td>9.369</td>\n",
       "      <td>8.834</td>\n",
       "      <td>8.518</td>\n",
       "      <td>8.349</td>\n",
       "      <td>8.291</td>\n",
       "      <td>8.24</td>\n",
       "      <td>8.122</td>\n",
       "      <td>8.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      1Y     2Y     3Y     4Y     5Y     6Y    7Y     8Y     9Y  \\\n",
       "0 1974-09-24  10.327  9.362  8.830  8.515  8.348  8.290  8.24  8.121  8.127   \n",
       "1 1974-09-25  10.333  9.364  8.831  8.516  8.348  8.290  8.24  8.121  8.127   \n",
       "2 1974-09-26  10.340  9.366  8.832  8.516  8.348  8.290  8.24  8.122  8.128   \n",
       "3 1974-09-27  10.347  9.367  8.833  8.517  8.349  8.290  8.24  8.122  8.128   \n",
       "4 1974-09-28  10.354  9.369  8.834  8.518  8.349  8.291  8.24  8.122  8.129   \n",
       "\n",
       "   10Y  15Y  20Y  25Y  30Y  40Y  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = \"https://www.mof.go.jp/english/policy/jgbs/reference/interest_rate/historical/jgbcme_all.csv\"\n",
    "response = requests.get(url, timeout=30)\n",
    "response.encoding = 'shift_jis'\n",
    "\n",
    "df = pd.read_csv(StringIO(response.text), header=1, na_values=\"-\",parse_dates=[\"Date\"])\n",
    "df.head(5)\n",
    "# 10年国債利回りを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fca4f-1b5f-4ce3-a2f5-b415ec8cca01",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:23:19.0255454Z\",\"execution_finish_time\":\"2026-02-15T10:23:43.0770853Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": "\n# ============================================================\n# Cell 2: pandas DataFrame → Spark DataFrame → Delta table 書き込み\n# ============================================================\n\nfrom pyspark.sql.types import StructType, StructField, DateType, DoubleType, TimestampType, IntegerType\n\n# --- 明示的スキーマ定義（設計書準拠）---\nschema = StructType([\n    StructField(\"date\",        DateType(),      nullable=False),\n    StructField(\"yield_10y\",   DoubleType(),    nullable=False),\n    StructField(\"ingested_at\", TimestampType(), nullable=False),\n    StructField(\"year\",        IntegerType(),   nullable=False),\n    StructField(\"month\",       IntegerType(),   nullable=False),\n])\n\n# --- スキーマ定義と同じ順序でカラムを並べる（位置マッピングのずれを防止）---\ndf_ordered = df[[\"date\", \"yield_10y\", \"ingested_at\", \"year\", \"month\"]]\n\n# --- pandas → Spark DataFrame 変換 ---\nspark_df = spark.createDataFrame(df_ordered, schema=schema)\n\n# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\n(\n    spark_df\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .partitionBy(\"year\", \"month\")\n    .option(\"overwriteSchema\", \"true\")\n    .saveAsTable(TABLE_NAME)\n)\n\nactual_count = spark.read.table(TABLE_NAME).count()\nprint(f\"[完了] Delta table '{TABLE_NAME}' に {actual_count:,} 行を書き込みました\")\nprint(f\"パーティション: year / month\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102da8f-e16c-4880-a4d5-a20e4d21396f",
   "metadata": {
    "cellStatus": "{\"南勇輝\":{\"session_start_time\":null,\"execution_start_time\":\"2026-02-15T10:27:40.0943652Z\",\"execution_finish_time\":\"2026-02-15T10:27:41.4843815Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}",
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T04:22:23.5780253Z",
       "execution_start_time": "2026-02-22T04:21:57.3535121Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c34445a7-f390-44ce-8621-ced3f08fb634",
       "queued_time": "2026-02-22T04:21:57.0301172Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2026-02-22T04:22:22.654GMT",
          "dataRead": 103756,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5))",
          "displayName": "getRowsInJsonString at Display.scala:422",
          "jobGroup": "6",
          "jobId": 14,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "getRowsInJsonString at Display.scala:422",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 1644,
          "stageIds": [
           22
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:22.240GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:22.044GMT",
          "dataRead": 4760,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 13,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           19,
           20,
           21
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:21.889GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:21.815GMT",
          "dataRead": 5917,
          "dataWritten": 4760,
          "description": "Delta: Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query: Compute snapshot for version: 0",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 12,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 60,
          "stageIds": [
           17,
           18
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:20.621GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:20.212GMT",
          "dataRead": 5469,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 11,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 11,
          "stageIds": [
           15,
           16
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:17.033GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:15.860GMT",
          "dataRead": 10212,
          "dataWritten": 5469,
          "description": "Delta: Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5)): Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 10,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 22,
          "stageIds": [
           14
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:15.634GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2026-02-22T04:22:13.618GMT",
          "dataRead": 10212,
          "dataWritten": 0,
          "description": "Job group for statement 6:\ndf = spark.read.table(\"bronze_interest_rate\")\ndisplay(df.limit(5))",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "6",
          "jobId": 9,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 11,
          "stageIds": [
           13
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2026-02-22T04:22:13.064GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 6,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.widget-view+json": {
       "widget_id": "4aee22ec-fa7e-4fa1-876c-9a53000b7a7d",
       "widget_type": "Synapse.DataFrame"
      },
      "text/plain": [
       "SynapseWidget(Synapse.DataFrame, 4aee22ec-fa7e-4fa1-876c-9a53000b7a7d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pandas DataFrame → Spark DataFrame に変換し、Delta table として書き込み\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "table_name = \"bronze_interest_rate\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"dbo.{table_name}\")\n",
    "\n",
    "print(f\"Delta table '{table_name}' に {spark_df.count()} 行を書き込みました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3116a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T05:01:01.3646038Z",
       "execution_start_time": "2026-02-22T05:01:01.0770949Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a30135fd-760a-4b5b-9d77-2166d7978401",
       "queued_time": "2026-02-22T05:01:01.075905Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 200,
       "statement_ids": [
        200
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 200, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---型変換---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, IntegerType, TimestampType \n",
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"10Y\", DoubleType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"ingested_at\", TimestampType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32043831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-22T05:01:01.8667614Z",
       "execution_start_time": "2026-02-22T05:01:01.5886155Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "80ec02a9-3cee-471e-b0a8-a839349306a9",
       "queued_time": "2026-02-22T05:01:01.5874226Z",
       "session_id": "3ed54005-e5ae-4583-9176-6827b8e4ca03",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 201,
       "statement_ids": [
        201
       ]
      },
      "text/plain": [
       "StatementMeta(, 3ed54005-e5ae-4583-9176-6827b8e4ca03, 201, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported cast from double to timestamp using function cast_timestamp\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `9.362` in type `float`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[602], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m TABLE_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_interest_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --- pandas → Spark DataFrame 変換 ---\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\u001b[39;00m\n\u001b[1;32m      6\u001b[0m (\n\u001b[1;32m      7\u001b[0m     spark_df\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39msaveAsTable(TABLE_NAME)\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     verify_func(obj)\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2174\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2165\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2166\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2171\u001b[0m             },\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2174\u001b[0m         verifier(v)\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2176\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2201\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2201\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2090\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_integer\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2089\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2090\u001b[0m     verify_acceptable_types(obj)\n\u001b[1;32m   2091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2147483648\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2147483647\u001b[39m:\n\u001b[1;32m   2092\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2093\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_OUT_OF_BOUND\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2094\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             },\n\u001b[1;32m   2100\u001b[0m         )\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py:2020\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2020\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2021\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2022\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2023\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2024\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2025\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2026\u001b[0m             },\n\u001b[1;32m   2027\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `9.362` in type `float`."
     ]
    }
   ],
   "source": [
    "TABLE_NAME = \"bronze_interest_rate\"\n",
    "# --- pandas → Spark DataFrame 変換 ---\n",
    "spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "# --- Delta table 書き込み（overwrite: 再実行時も冪等性を保証）---\n",
    "(\n",
    "    spark_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(TABLE_NAME)\n",
    ")\n",
    "\n",
    "actual_count = spark.read.table(TABLE_NAME).count()\n",
    "print(f\"[完了] Delta table '{TABLE_NAME}' に {actual_count:,} 行を書き込みました\")\n",
    "print(f\"パーティション: year / month\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bb192-016c-476f-8c01-2e2023e9952c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "daafb4a7-f64d-4a0d-b5b6-a3f7d2e6f3ab",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "a0023aa2-9b7b-419b-bec4-ad2bc8e7db13",
    "known_lakehouses": [
     {
      "id": "daafb4a7-f64d-4a0d-b5b6-a3f7d2e6f3ab"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "36d7f2a3-89c7-4786-bb95-a79a13fbe5e4": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "R1.5.13",
         "1": "-0.158",
         "2": "-0.159",
         "3": "-0.163",
         "4": "-0.169",
         "5": "-0.164",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.09",
         "10": "-0.043",
         "11": "0.175",
         "12": "0.375",
         "13": "0.463",
         "14": "0.539",
         "15": "0.592"
        },
        {
         "0": "R1.5.14",
         "1": "-0.159",
         "2": "-0.159",
         "3": "-0.163",
         "4": "-0.17",
         "5": "-0.164",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.094",
         "10": "-0.048",
         "11": "0.17",
         "12": "0.365",
         "13": "0.456",
         "14": "0.534",
         "15": "0.588"
        },
        {
         "0": "R1.5.15",
         "1": "-0.16",
         "2": "-0.16",
         "3": "-0.163",
         "4": "-0.17",
         "5": "-0.165",
         "6": "-0.167",
         "7": "-0.163",
         "8": "-0.135",
         "9": "-0.094",
         "10": "-0.048",
         "11": "0.17",
         "12": "0.365",
         "13": "0.453",
         "14": "0.53",
         "15": "0.588"
        },
        {
         "0": "R1.5.16",
         "1": "-0.16",
         "2": "-0.166",
         "3": "-0.174",
         "4": "-0.179",
         "5": "-0.175",
         "6": "-0.177",
         "7": "-0.169",
         "8": "-0.145",
         "9": "-0.104",
         "10": "-0.058",
         "11": "0.154",
         "12": "0.35",
         "13": "0.437",
         "14": "0.511",
         "15": "0.567"
        },
        {
         "0": "R1.5.17",
         "1": "-0.161",
         "2": "-0.164",
         "3": "-0.169",
         "4": "-0.175",
         "5": "-0.169",
         "6": "-0.172",
         "7": "-0.161",
         "8": "-0.137",
         "9": "-0.099",
         "10": "-0.053",
         "11": "0.165",
         "12": "0.356",
         "13": "0.447",
         "14": "0.525",
         "15": "0.582"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "基準日",
         "type": "string"
        },
        {
         "key": "1",
         "name": "1年",
         "type": "string"
        },
        {
         "key": "2",
         "name": "2年",
         "type": "string"
        },
        {
         "key": "3",
         "name": "3年",
         "type": "string"
        },
        {
         "key": "4",
         "name": "4年",
         "type": "double"
        },
        {
         "key": "5",
         "name": "5年",
         "type": "double"
        },
        {
         "key": "6",
         "name": "6年",
         "type": "double"
        },
        {
         "key": "7",
         "name": "7年",
         "type": "double"
        },
        {
         "key": "8",
         "name": "8年",
         "type": "double"
        },
        {
         "key": "9",
         "name": "9年",
         "type": "double"
        },
        {
         "key": "10",
         "name": "10年",
         "type": "string"
        },
        {
         "key": "11",
         "name": "15年",
         "type": "string"
        },
        {
         "key": "12",
         "name": "20年",
         "type": "string"
        },
        {
         "key": "13",
         "name": "25年",
         "type": "string"
        },
        {
         "key": "14",
         "name": "30年",
         "type": "string"
        },
        {
         "key": "15",
         "name": "40年",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}